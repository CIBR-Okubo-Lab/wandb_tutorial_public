{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb tutorial\n",
    "\n",
    "Yue Chen, Xin Zheng, and Tatsuo Okubo\n",
    "\n",
    "2024/07/03\n",
    "\n",
    "This notebook shows the basic functions of Wandb sweeps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab preparation\n",
    "\n",
    "Please run the code below if you are using Google Colab.\n",
    "\n",
    "There is no need to run the code if you are using a local machine or server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CIBR-Okubo-Lab/wandb_tutorial_public.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/content/wandb_tutorial_public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start sweeps tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "from utils import create_dataloaders, cnn, train_epoch, eval_epoch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb login\n",
    "To login to wandb, visit https://wandb.ai/authorize and copy your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='PASTE YOUR API KEY HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1Ô∏è: Define the search space with a sweep configuration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sweep will try numerous combinations of hyperparameters with multiple runs. To run a sweep, the first step is to define the search space with a sweep configuration.\n",
    "\n",
    "A sweep configuration can be defined in a nested dictionary for Jupyter Notbook or a [YAML](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration) file for scripts. In this tutorial, we will use the nested dictionary.\n",
    "\n",
    "\n",
    "Essential keys in a configuration: \n",
    "1. `name` (**optional**): the name of the sweep\n",
    "\n",
    "1. `method` (**required**): the hyperparameter search strategy. You can choose among `grid`, `random`, and `bayes`.\n",
    "\n",
    "2. `metric` (**optional**): the metric to monitor. This is optional for grid and random search but required for Bayesian search. It will be shown as the last column of the parallel coordinates chart. \n",
    "\n",
    "3. `parameters` (**required**): the hyperparameters to tune. Each hyperparameter can be assigned a list of values to choose from or a distribution to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmple of sweep config\n",
    "sweep_config = {\n",
    "    'name': 'random_exp',\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val/acc',\n",
    "        'goal': 'maximize'  \n",
    "    },\n",
    "    'parameters': {\n",
    "        'hidden_layer_width': {\n",
    "            'values': [32, 64, 128, 256],\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0.0, 0.2, 0.4, 0.6, 0.8],\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 5\n",
    "        },\n",
    "        'lr': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-6,\n",
    "            'max': 0.1,\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [128, 256, 512, 1024]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print configuration \n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 5: Define a sweep configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define a sweep configuration with `name`, `method`, and `metric`\n",
    "\n",
    "* **name**: Assign a meaningful name, such as \"mnist_random\".\n",
    "\n",
    "* **method**: Use `random` search for this exercise. \n",
    "\n",
    "* **metric**: Specify the `metric` with `name` being `\"val/acc\"` and `goal` being `\"maximize\"`. \n",
    "\n",
    "* Defining `metric` will allow wandb to automatically create the following: \n",
    "\n",
    "    * a \"metric vs. created time\" scatter plot\n",
    "\n",
    "    * a hyperparameter importance plot\n",
    "\n",
    "    * the last column of the parallel coordinates chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sweep configuration\n",
    "sweep_config = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Update hyperparameter space to search with learning rate. \n",
    "\n",
    "* Given the existing hyperparameter configuration, update it to include the learning rate. The key for learning rate is `\"lr\"`. \n",
    "    * `\"distribution\"`: \"log_uniform_values\"\n",
    "\n",
    "    * `\"min\"`: 1e-6\n",
    "    \n",
    "    * `\"max\"`: 0.1 \n",
    "\n",
    "* Update the sweep configuration with this hyperparameter search space. The key in sweep_config is \"parameters\".\n",
    "\n",
    "You can print out `sweep_config` to verify it matches your expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter configuration\n",
    "parameters_dict = {\n",
    "    'hidden_layer_width': {\n",
    "        'values': [32, 64, 128, 256],\n",
    "    },\n",
    "    'dropout_rate': {\n",
    "        'values': [0.0, 0.2, 0.4, 0.6, 0.8],\n",
    "    },\n",
    "    'epochs': {\n",
    "        'value': 5\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [128, 256, 512, 1024]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Modify the code below. \n",
    "'''\n",
    "# 1). update with learning rate\n",
    "parameters_dict['lr'] = {\n",
    "\n",
    "}\n",
    "\n",
    "# 2). update sweep config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Initialize the sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize a sweep, call `wandb.sweep()` and specify sweep configuration, project name, and entity (if applicable).\n",
    "\n",
    "A `sweep_id` will be returned and it can be used to `start` or `resume` the sweep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"wandb-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 6: Initialize a sweep\n",
    "\n",
    "* Initialize a sweep using the configuration you just defined. Specify the project name. \n",
    "\n",
    "* Assign the returned sweep id to a variable named `sweep_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a sweep here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the training method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your training code, you need to initialize wandb runs. \n",
    "\n",
    "The run configuration will be handled by sweep controller, so there's no need to pass the run configuration, project name, or entity in `wandb.init()`. \n",
    "\n",
    "Retrieve the configuration from `wandb.config` to use the hyperparameter values in the training code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    \n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "    # config for specific runs will be handled by Wandb Controller\n",
    "    wandb.init(name=nowtime)\n",
    "    config = wandb.config\n",
    "\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=config['lr'])\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")   \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 7: Modify the training code to initialize wandb runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize W&B runs using `wandb.init()`and specify the run name with `nowtime` variable. \n",
    "\n",
    "2. Retreive the configuration using `wandb.config` and assign it to a variable named `config`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    \n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "    # config for specific runs will be handled by Wandb Controller\n",
    "    '''\n",
    "    Initialize wandb run here.\n",
    "    '''\n",
    "\n",
    "\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")   \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: use agent to start the sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `wandb.agent()` to start the sweep. Specify the following:\n",
    "\n",
    "* `sweep_id`: sweep id \n",
    "\n",
    "* `function`: specify the function you want the agent to run.\n",
    "\n",
    "* `count`: determine the number of times to run the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will start the sweep.\n",
    "# wandb.agent(sweep_id=sweep_id, function=train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 8: Run training code with agent\n",
    "\n",
    "* Use `wandb.agent()` to start runs in sweep\n",
    "\n",
    "* Specify `sweep_id`, `function`, and `count`\n",
    "\n",
    "* If you have access to GPU, set `count` to 20. If you are using CPU, set `count` to 5. If you are using Google Colab, set `count` to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the sweep using agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 9 (Optional): Run sweep with bayes search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name': 'mnist_bayes',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val/acc',\n",
    "        'goal': 'maximize'  \n",
    "    },\n",
    "    'parameters': {\n",
    "        'hidden_layer_width': {\n",
    "            'values': [32, 64, 128, 256],\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0.0, 0.2, 0.4, 0.6, 0.8],\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 5\n",
    "        },\n",
    "        'lr': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-6,\n",
    "            'max': 0.1,\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [128, 256, 512, 1024]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"wandb-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    \n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "    # config for specific runs will be handled by wandb controller\n",
    "    '''\n",
    "    Here is the answer\n",
    "    '''\n",
    "    wandb.init(name=nowtime)\n",
    "    config = wandb.config\n",
    "\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=config['lr'])\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")   \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
