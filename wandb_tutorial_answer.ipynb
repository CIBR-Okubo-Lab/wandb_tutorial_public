{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb tutorial (with answers)\n",
    "\n",
    "Yue Chen, Xin Zheng, and Tatsuo Okubo\n",
    "\n",
    "2024/06/26\n",
    "\n",
    "This is the notebook for showing basic functions of `wandb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Preparation (terminal)\n",
    "\n",
    "We need to install `wandb`, register an account and log in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install\n",
    "\n",
    "Install `wandb` using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to wandb account\n",
    "\n",
    "- Visit the [wandb website](https://wandb.ai/) to sign up for an account\n",
    "- Get the [API key](https://wandb.ai/authorize) and copy it to the clipboard\n",
    "- Login (paste the API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to change an account, please force relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login --relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize wandb and record configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment example\n",
    "\n",
    "Below is an example PyTorch training code on MNIST dataset without `wandb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "from utils import create_dataloaders, cnn, train_epoch, eval_epoch, show_cases  # utility functions for the tutorial\n",
    "\n",
    "# Settings\n",
    "config = {'batch_size': 512,\n",
    "          'hidden_layer_width': 64, \n",
    "          'dropout_rate': 0.2,\n",
    "          'lr': 1e-4,\n",
    "          'optimizer': 'Adam',\n",
    "          'epochs': 20}\n",
    "\n",
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize run\n",
    "\n",
    "We need to use [`wandb.init()`](https://docs.wandb.ai/ref/python/init) method to start a new run to track and log to W&B  \n",
    "\n",
    "During this step, we can specify project, group, name etc., we can also save the configurations.\n",
    "\n",
    "We can also control wandb with [environment variables](https://docs.wandb.ai/guides/track/environment-variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"WANDB_ENTITY\"] = 'okubo-lab-org'\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of an experiment, before training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1\n",
    "\n",
    "Please modify the code to specify the project name and run name as following:\n",
    "- project name: \"wandb_demo\"\n",
    "- run name: current time (`nowtime`)\n",
    "\n",
    "Please also record experiment hyperparameters recorded in the Python dictionary `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    '''\n",
    "    Please modify here\n",
    "    '''\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the code, please uncomment the last line and run the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    '''\n",
    "    Here is the answer\n",
    "    '''\n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to the wandb page.\n",
    "- Can you see the training loss curve and the validation accuracy curve? Why not?\n",
    "- What OS are you using? What Python version are you using? Check at `Overview` tab on the left.\n",
    "- You can check the output of the print function in `Logs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Track experimental results\n",
    "\n",
    "We can use [`wandb.log()`](https://docs.wandb.ai/ref/python/log) to log a dictionary of data to the current run's history.  \n",
    "\n",
    "The most basic usage is to provide a Python dictionary `{\"name\": value}` to `wandb.log()` function, for example `wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.   \n",
    "Note that compared to configuration that don't change within a single experiment, these values can get dynamically updated during the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2\n",
    "\n",
    "Please modify the code to record training loss, validation loss, and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")\n",
    "        \n",
    "        '''\n",
    "        Please modify here\n",
    "        '''    \n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the code, please uncomment the last line and run the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")\n",
    "        \n",
    "        '''\n",
    "        Here is the answer\n",
    "        '''    \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Version management\n",
    "\n",
    "We can use [`wandb.Artifact`](https://docs.wandb.ai/ref/python/artifact) to save experiment-related datasets, codes, and models to the server. It is very convenient for us or others to reproduce the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")  \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    arti_dataset = wandb.Artifact('mnist', type='dataset')\n",
    "    arti_dataset.add_dir('data/')\n",
    "    wandb.log_artifact(arti_dataset)\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 3\n",
    "\n",
    "Please modify the code to save the Jupyter notebook file.\n",
    "\n",
    "Hint:\n",
    "- Create a new `Artifact` object, specify name and type.\n",
    "- Use `add_file` instead of `add_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")  \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    '''\n",
    "    Please modify here\n",
    "    '''     \n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the code, please uncomment the last line and run the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")  \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    '''\n",
    "    Here is the answer\n",
    "    '''\n",
    "    arti_code = wandb.Artifact('ipynb', type='code')\n",
    "    arti_code.add_file('wandb_tutorial_answer.ipynb')\n",
    "    wandb.log_artifact(arti_code) \n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Case analysis\n",
    "\n",
    "Using [`wandb.Table`](https://docs.wandb.ai/guides/tables), we can perform interactive visual case analysis on dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "```\n",
    "my_table = wandb.Table(columns=[\"a\", \"b\"], data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]])\n",
    "wandb.log({\"Table Name\": my_table})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 4\n",
    "\n",
    "Please modify the code to create and log a table to show 10 validation results.\n",
    "\n",
    "The table should include:\n",
    "- Image: original input image\n",
    "- Target: target class of the image\n",
    "- Prediction: predicted class of the image\n",
    "\n",
    "Notice the `results` have already been formatted using `show_cases`  \n",
    "`results` format:  \n",
    "- list of lists\n",
    "- [n_cases*[Image, Target, Prediction]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = show_cases(model, show_num=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")  \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    \n",
    "    # results to show\n",
    "    results = show_cases(model, 10)\n",
    "    '''\n",
    "    Please modify here\n",
    "    '''\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the code, please uncomment the last line and run the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_cases\n",
    "\n",
    "def train(config):\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    model = cnn(config) \n",
    "    optimizer = torch.optim.__dict__[config['optimizer']](params=model.parameters(), lr=config['lr'])\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    wandb.init(project='wandb_demo', name=nowtime, config=config)\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        model, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_acc, val_loss = eval_epoch(model, test_loader)\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}, val_acc= {100 * val_acc:.2f}%\")  \n",
    "        wandb.log({\"train/loss\": train_loss, \"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "\n",
    "    \n",
    "    # results to show\n",
    "    results = show_cases(model, 10)\n",
    "    '''\n",
    "    Here is the answer\n",
    "    '''\n",
    "    column_name = ['Image', 'Target', 'Prediction']\n",
    "    cases = wandb.Table(columns=column_name, data=results)\n",
    "    wandb.log({'cases':cases})\n",
    "\n",
    "    wandb.finish() # Notify wandb that your run has ended and upload all log data to wandb\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
